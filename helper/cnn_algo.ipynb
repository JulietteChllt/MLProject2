{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import get_pre_process_data_test\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Flatten, MaxPooling1D, GRU, SpatialDropout1D, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_full.txt'\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_full.txt'\n",
    "\n",
    "# Load the preprocessed datasets already computed\n",
    "\n",
    "def get_input() :\n",
    "    with open(PATH_TRAIN_POS) as f:\n",
    "        train_pos = f.read().splitlines()\n",
    "    with open(PATH_TRAIN_NEG) as f:\n",
    "        train_neg = f.read().splitlines()\n",
    "\n",
    "    train_set = train_pos + train_neg\n",
    "\n",
    "    y = np.array(len(train_pos) * [1] + len(train_neg) * [0])\n",
    "\n",
    "    test_set = get_pre_process_data_test(save_file_name='test_data_process.txt')\n",
    "\n",
    "    return train_set, y, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_lstm(vocabulary_size, max_length) :\n",
    "    '''\n",
    "        compute the model using one embedding layer to put the tweet in spatial space,\n",
    "        make the lstm and one layer of dense with sigmoid activation to get the output in the range [0,1]\n",
    "\n",
    "        INPUTS : \n",
    "            vocabulary_size : number of different words\n",
    "            max_length : size of one vector tweet in X\n",
    "        OUTPUTS :\n",
    "            The model ready to be trained\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(vocabulary_size, 200, input_length=max_length))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, vocabulary_size, max_length, epochs=100) :\n",
    "    '''\n",
    "        Train the model with the sentiments of the train_set\n",
    "        INPUTS :\n",
    "            model : model to be trained\n",
    "            X : the vectorize form of the train set\n",
    "            y : the sentiment of each tweet in X\n",
    "            vocabulary_size : number of different words\n",
    "            max_length : size of one vector tweet in X\n",
    "\n",
    "        OUTPUTS :\n",
    "            the trained model\n",
    "    '''\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(sequences, maxlen=max_length)\n",
    "    model.fit(X,y, epochs=epochs)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test_dataset, tokenizer,max_length) :\n",
    "    '''\n",
    "        Put the test tweets in vector forms and predict them with the model\n",
    "        OUTPUTS :\n",
    "            the predictions, each predictions is in the range [0,1]\n",
    "    '''\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_dataset)\n",
    "    test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "    return model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(predictions) :\n",
    "    '''\n",
    "        write the predictions in the glove_results file\n",
    "        INPUTS :\n",
    "            prediction : 10 000 sentiments of the test tweets in range [0,1]\n",
    "    '''\n",
    "    predictions =list(zip(range(1, 10001),predictions))\n",
    "    with open('../Resources/cnn_lstm.csv', 'w') as out:\n",
    "        writer = csv.writer(out)\n",
    "        writer.writerow([\"Id\", \"Prediction\"])\n",
    "        for a,b in predictions:\n",
    "            if b < 0.5:\n",
    "                writer.writerow([a, -1])\n",
    "            else:\n",
    "                writer.writerow([a, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_lstm() : \n",
    "    max_length = 32 \n",
    "    vocabulary_size = 100000\n",
    "\n",
    "    # load the datasets\n",
    "    X, y, test = get_input()\n",
    "\n",
    "    # make and train the model\n",
    "    model = get_model_cnn_lstm(max_length=max_length, vocabulary_size=vocabulary_size)\n",
    "    model, toke = train_model(model, X, y, max_length=max_length, vocabulary_size=vocabulary_size)\n",
    "    \n",
    "    # make the predictions of our test dataset with our model\n",
    "    predictions = make_predictions(model, test, toke, max_length)\n",
    "    make_submission(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 5142/64597 [=>............................] - ETA: 134:49:23 - loss: 0.4542 - accuracy: 0.7784"
     ]
    }
   ],
   "source": [
    "run_cnn_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
