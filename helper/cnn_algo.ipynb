{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import get_pre_process_data_test\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Flatten, MaxPooling1D, GRU, SpatialDropout1D, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_full.txt'\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_full.txt'\n",
    "\n",
    "# Load the preprocessed datasets already computed\n",
    "\n",
    "def get_input() :\n",
    "    with open(PATH_TRAIN_POS) as f:\n",
    "        train_pos = f.read().splitlines()\n",
    "    with open(PATH_TRAIN_NEG) as f:\n",
    "        train_neg = f.read().splitlines()\n",
    "\n",
    "    train_set = train_pos + train_neg\n",
    "\n",
    "    y = np.array(len(train_pos) * [1] + len(train_neg) * [0])\n",
    "\n",
    "    test_set = get_pre_process_data_test(save_file_name='test_data_process.txt')\n",
    "\n",
    "    return train_set, y, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_lstm(vocabulary_size, max_length) :\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(vocabulary_size, 200, input_length=max_length))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, vocabulary_size, max_length, epochs=100) :\n",
    "    tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(sequences, maxlen=max_length)\n",
    "    model.fit(X,y, epochs=epochs)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test_dataset, tokenizer,max_length) :\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_dataset)\n",
    "    test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "    return model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(predictions) :\n",
    "    '''\n",
    "        write the predictions in the glove_results file\n",
    "        INPUTS :\n",
    "            prediction : 10 000 sentiments of the test tweets in range [0,1]\n",
    "    '''\n",
    "    predictions =list(zip(range(1, 10001),predictions))\n",
    "    with open('../Resources/cnn_lstm.csv', 'w') as out:\n",
    "        writer = csv.writer(out)\n",
    "        writer.writerow([\"Id\", \"Prediction\"])\n",
    "        for a,b in predictions:\n",
    "            if b < 0.5:\n",
    "                writer.writerow([a, -1])\n",
    "            else:\n",
    "                writer.writerow([a, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_lstm() :\n",
    "    max_length = 32\n",
    "    vocabulary_size = 100000\n",
    "    X, y, test = get_input()\n",
    "    model = get_model_cnn_lstm(max_length=max_length, vocabulary_size=vocabulary_size)\n",
    "    model, toke = train_model(model, X, y, max_length=max_length, vocabulary_size=vocabulary_size)\n",
    "    predictions = make_predictions(model, test, toke, max_length)\n",
    "    make_submission(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 5142/64597 [=>............................] - ETA: 134:49:23 - loss: 0.4542 - accuracy: 0.7784"
     ]
    }
   ],
   "source": [
    "run_cnn_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
