{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b957f9e",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669d86a",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a292d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import keras                                                            #pip install keras #pip install tensorflow\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec                                   #pip install gensim\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import gensim.downloader as api\n",
    "from glove import Corpus, Glove                                      #pip install glove-py\n",
    "from joblib import dump, load\n",
    "from gensim import utils\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/train_neg_processed.txt'\n",
    "PATH_TRAIN_POS = '../Resources/train_pos_processed.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS) as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG) as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "\n",
    "train_set = train_pos + train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f302c3",
   "metadata": {},
   "source": [
    "## Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dda91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"basic and naive method for word embedding\"\n",
    "def we_count_vectorize(train_set) :\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    text_counts = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "    \n",
    "    #df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    #df = df.sort_values('Count frequency', ascending=False)\n",
    "    #print(df.head(25))\n",
    "    \n",
    "    return voc, text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ffb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Sparse matrix : would require way too much space (around 38GB)\"\n",
    "voc, text_counts = we_count_vectorize(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a532f5",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41190214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_tfIdf(train_set,n_min,n_max) : \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(n_min, n_max))\n",
    "    text_tfIdf = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "\n",
    "    #df = pd.DataFrame(text_tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    #df = df.sort_values('TF-IDF', ascending=False)\n",
    "    #print (df.head(25))\n",
    "    \n",
    "    return voc, text_tfIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25194ad",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e03c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, positive_corpus,negative_corpus):\n",
    "        \n",
    "        self.positive_corpus = positive_corpus\n",
    "        self.negative_corpus = negative_corpus\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in open(self.positive_corpus):            \n",
    "            yield utils.simple_preprocess(line)\n",
    "        for line in open(self.negative_corpus):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f686d",
   "metadata": {},
   "source": [
    "#### do not run this cell again : run the next one to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neural embedding model\n",
    "\"\"\"\n",
    "def we_word2vector_create_model(path_train_pos, path_train_neg) :\n",
    "    \n",
    "    corpus = MyCorpus(path_train_pos, path_train_neg)\n",
    "    model = Word2Vec(sentences = corpus)\n",
    "    model.save('word2vec_saved_model.joblib')\n",
    "    model.wv.save(\"word2vec.wordvectors\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2649693",
   "metadata": {},
   "source": [
    "#### load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a50290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path_train_pos = ..\\\\Resources\\\\train_pos.txt\n",
    "# for path_train_heg = ..\\\\Resources\\\\train_neg.txt\n",
    "def we_word2vector_load_model() :\n",
    "    return gensim.models.Word2Vec.load('word2vec_saved_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4030853",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d81f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set):\n",
    "    corpus=[]\n",
    "    for tweet in train_set:\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if(len(tweet)>1 and word.isalpha())]\n",
    "        corpus.append(words)\n",
    "        corpus = [element for element in corpus if len(element)>0]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ab9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe pretrained twitter dataset\n",
    "def load_pretrained():\n",
    "    embedding_dict={}\n",
    "    with open('../Resources/twitter_dict/glove.twitter.27B.200d.txt','rb') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[values[0]]=vectors\n",
    "    f.close()\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f329a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enbedding_dict = load_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbc9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(corpus, max_length=None) : \n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = len(word_index)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    tweet_pad = pad_sequences(sequences,maxlen=max_length,truncating='post',padding='post')\n",
    "    \n",
    "    corpus_size = len(corpus)\n",
    "    y = np.array(int(corpus_size/2) * [0] + int(corpus_size/2) * [1])\n",
    " \n",
    "    indices = np.arange(tweet_pad.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    tweet_pad = tweet_pad[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    return tweet_pad, word_index, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a091f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(train_set)\n",
    "seq, wi,y = create_sequence(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aed24e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_glove(train_set, word_index):\n",
    "    num_words=len(word_index)+1\n",
    "    tweets = [tweet.split() for tweet in train_set]\n",
    "    embedding_dict = load_pretrained()\n",
    "    glove_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "    for word,i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "    \n",
    "        emb_vec=embedding_dict.get(word.encode())\n",
    "        if emb_vec is not None:\n",
    "            glove_matrix[i]=emb_vec\n",
    "            \n",
    "    return glove_matrix, word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1360638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix, word_embedding = we_glove(train_set, wi)\n",
    "num_words = len(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02931823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69234, 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
