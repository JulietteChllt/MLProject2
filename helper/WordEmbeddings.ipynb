{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b957f9e",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669d86a",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a292d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import necessary libraries\n",
    "\"\"\"\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import keras                                                            #pip install keras #pip install tensorflow\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import Word2Vec                                   #pip install gensim\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim import utils\n",
    "from glove import Corpus, Glove                                      #pip install glove-python-binary \n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load full train data processed and build train set \n",
    "\"\"\"\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_fp_full.txt'\n",
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_fp_full.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS,errors = 'ignore') as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG,errors = 'ignore') as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "\n",
    "train_set = train_pos + train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f302c3",
   "metadata": {},
   "source": [
    "## Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dda91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basic and naive method for word embedding\n",
    "\"\"\"\n",
    "def we_count_vectorize(train_set) :\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    text_counts = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "    \n",
    "    #df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"count vectorize\"])\n",
    "    #df = df.sort_values('Count frequency', ascending=False)\n",
    "    #print(df.head(25))\n",
    "    \n",
    "    return voc, text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ffb8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sparse matrix : would require way too much space (around 38GB)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Sparse matrix : would require way too much space (around 38GB)\"\n",
    "voc, text_counts = we_count_vectorize(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a532f5",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41190214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUT : \n",
    "    train_set : list         - contains all the positive and negative tweets\n",
    "    n_min : int              - the minimal number or words in the word representation of the vocabulary (set to one)\n",
    "    n_max : int              - maximal number of words : chose 3 for 3-gram, 4 for 4-gram...\n",
    "\"\"\"\n",
    "def we_tfIdf(train_set,n_min,n_max) : \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(n_min, n_max))\n",
    "    text_tfIdf = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "\n",
    "    #df = pd.DataFrame(text_tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    #df = df.sort_values('TF-IDF', ascending=False)\n",
    "    #print (df.head(25))\n",
    "    \n",
    "    return voc, text_tfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890ffba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_tf, tf_idf = we_tfIdf(train_set,1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25194ad",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42e03c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Word2vector corpus \n",
    "\"\"\"\n",
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, positive_corpus,negative_corpus):\n",
    "        \n",
    "        self.positive_corpus = positive_corpus\n",
    "        self.negative_corpus = negative_corpus\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in open(self.positive_corpus):            \n",
    "            yield utils.simple_preprocess(line)\n",
    "        for line in open(self.negative_corpus):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f686d",
   "metadata": {},
   "source": [
    "#### do not run this cell again : run the next one to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neural embedding model : saves and return Word2vector model\n",
    "    INPUT : \n",
    "        path_train_pos: string     \n",
    "        path_train_neg: string\n",
    "    OUTPUT : \n",
    "        returns the Word2vector model on the corpus \n",
    "\"\"\"\n",
    "def we_word2vector_create_model(path_train_pos, path_train_neg) :\n",
    "    \n",
    "    corpus = MyCorpus(path_train_pos, path_train_neg)\n",
    "    model = Word2Vec(sentences = corpus)\n",
    "    model.save('word2vec_saved_model.joblib')\n",
    "    model.wv.save(\"word2vec.wordvectors\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2649693",
   "metadata": {},
   "source": [
    "#### load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f2a50290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load the Word2Vector model previously saved\n",
    "\"\"\"\n",
    "# for path_train_pos = ..\\\\Resources\\\\train_pos.txt\n",
    "# for path_train_heg = ..\\\\Resources\\\\train_neg.txt\n",
    "def we_word2vector_load_model() :\n",
    "    return gensim.models.Word2Vec.load('word2vec_saved_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4030853",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0d81f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create glove corpus from training set \n",
    "\"\"\"\n",
    "def create_corpus(train_set):\n",
    "    \n",
    "    corpus=[]\n",
    "    for tweet in train_set:\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if(len(tweet)>1 and word.isalpha())]\n",
    "        corpus.append(words)\n",
    "        corpus = [element for element in corpus if len(element)>0]\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8dcfb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create embedding dictionary from GloVe pretrained twitter dataset\n",
    "here we use the one of dimension 200\n",
    "\"\"\"\"\n",
    "def create_pretrained() : \n",
    "    embedding_dict={}\n",
    "    with open('../Resources/twitter_dict/glove.twitter.27B.200d.txt','rb') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[values[0].decode()]=vectors\n",
    "    f.close()\n",
    "    \n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5c5bcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create embedding dictionary from our twitter dataset and save it as glove.model\n",
    "\"\"\"\"\n",
    "def create_glove_emb(train_set) :\n",
    "    model = Corpus()\n",
    "    train_splitted = [tweet.split() for tweet in train_set]\n",
    "    model.fit(train_splitted, window = 5)\n",
    "    \n",
    "    glove = Glove(no_components=200, learning_rate=0.05)\n",
    "    glove.fit(model.matrix, epochs=50)\n",
    "    glove.add_dictionary(model.dictionary)\n",
    "    glove.save('glove.model')\n",
    "    \n",
    "    embedding_dict = {}\n",
    "    for w, id_ in glove.dictionary.items():\n",
    "        embedding_dict[w] = np.array(glove.word_vectors[id_])\n",
    "\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "61ab9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load previously computed embedding dictionaries \n",
    "\"\"\"\n",
    "def load_embedding_dict(train_set, use_pretrained=True):\n",
    "    \n",
    "    if (use_pretrained) : \n",
    "        embedding_dict = create_pretrained()\n",
    "    else : \n",
    "        embedding_dict = create_glove_emb(train_set)\n",
    "    \n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2dbc9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sequences from corpus and tokenize corpus\n",
    "Pad sequences with zeros so they all have the same length (max_length)\n",
    "\"\"\"\n",
    "def create_sequence(corpus, max_length=None) : \n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = len(word_index)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    tweet_pad = pad_sequences(sequences,maxlen=max_length,truncating='post',padding='post')\n",
    "    \n",
    "    corpus_size = len(corpus)\n",
    "    y = np.array(int(corpus_size/2) * [0] + int(corpus_size/2) * [1])\n",
    " \n",
    "    indices = np.arange(tweet_pad.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    tweet_pad = tweet_pad[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    return tweet_pad , word_index , nb_words, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "aed24e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_glove(train_set,use_pretrained = True, max_length = None):\n",
    "    corpus = create_corpus(train_set)\n",
    "    sequence, word_index, nb_words, y = create_sequence(corpus,max_length)\n",
    "    num_words=nb_words+1\n",
    "    tweets = [tweet.split() for tweet in train_set]\n",
    "    embedding_dict = load_embedding_dict(train_set, use_pretrained)\n",
    "    glove_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "    for word,i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        if use_pretrained : \n",
    "            emb_vec = embedding_dict.get(word)\n",
    "        else : \n",
    "            emb_vec = embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            glove_matrix[i]=emb_vec\n",
    "            \n",
    "    return sequence, glove_matrix, embedding_dict, nb_words, y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120d80c",
   "metadata": {},
   "source": [
    "## test both pretrained and in house glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6e1ccc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, glove_matrix, embedding_dict, nb_words, y  = we_glove(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8df4084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1, glove_matrix1, embedding_dict1, nb_words1, y1  = we_glove(train_set,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "92c4c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_word(embedding_dict, emmbedes):\n",
    "    nearest = sorted(embedding_dict.keys(), key=lambda word: spatial.distance.euclidean(embedding_dict[word], emmbedes))\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "de0c639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'you',\n",
       " 'much',\n",
       " 'always',\n",
       " 'know',\n",
       " 'loves',\n",
       " 'miss',\n",
       " 'loving',\n",
       " 'true',\n",
       " 'life']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(embedding_dict, embedding_dict['love'])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9fce2686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'kiss\",\"positive',\n",
       " 'fan',\n",
       " 'beautiful',\n",
       " 'know\",\"personally',\n",
       " 'love\",\"positive',\n",
       " 'rain\",\"positive',\n",
       " 'justin\",\"wear',\n",
       " 'kt\",\"really',\n",
       " 'youu']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(embedding_dict1, embedding_dict1['love'])[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
