{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ba595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\julie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\julie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pre_processing import get_pre_process_data\n",
    "from pre_processing import get_pre_process_data_test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8438f",
   "metadata": {},
   "source": [
    "## get the data ready "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3381d",
   "metadata": {},
   "source": [
    "### first try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac3cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try n°1\n",
    "def desired_preprocessing(full, testing, save_file_name_pos,save_file_name_neg, save_file_name_test) : \n",
    "    positive = get_pre_process_data(positive = True, full=full, ponctuation=True, letter_repetition=True,\n",
    "                         emoji=True, hashtag=True, apostroph=True, slang=True, slang2=False,\n",
    "                         short_word=True, numbers=True, spelling=True, alphabetic=True,\n",
    "                         stopwords=True, stemming=False, lemmatizing=True, lemmatizing2=False, neg_pos_word=False,ekphrasis = False, save_file_name=save_file_name_pos)\n",
    "    \n",
    "    negative = get_pre_process_data(positive= False, full=full, ponctuation=True, letter_repetition=True,\n",
    "                         emoji=True, hashtag=True, apostroph=True, slang=True, slang2=False,\n",
    "                         short_word=True, numbers=True, spelling=True, alphabetic=True,\n",
    "                         stopwords=True, stemming=False, lemmatizing=True, lemmatizing2=False, neg_pos_word=False,ekphrasis = False, save_file_name=save_file_name_neg)\n",
    "    if(testing):\n",
    "        test = get_pre_process_data_test(ponctuation=True, letter_repetition=True,\n",
    "                              emoji=True, hashtag=True, apostroph=True, slang=True, slang2=False,\n",
    "                              short_word=True, numbers=True, spelling=True, alphabetic=True,\n",
    "                              stopwords=True, stemming=False, lemmatizing=True, lemmatizing2=False, neg_pos_word=False,ekphrasis = False, save_file_name=save_file_name_test)\n",
    "    else : \n",
    "        test = []\n",
    "    return positive, negative, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1010d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "tweet_pos, tweet_neg, tweet_test = desired_preprocessing(False,True,'preprocessing_pos1.txt','preprocessing_neg1.txt','preprocessing_test1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0be201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "tweet_pos_full, tweet_neg_full, garbage = desired_preprocessing(True, False,'preprocessing_pos_full.txt','preprocessing_neg_full.txt','garbage.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6a123",
   "metadata": {},
   "source": [
    "### 2nd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c62904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try n°2\n",
    "def desired_preprocessing2(full, testing, save_file_name_pos,save_file_name_neg, save_file_name_test) : \n",
    "    positive = get_pre_process_data(positive = True, full=full, ponctuation=True, letter_repetition=True,\n",
    "                         emoji=False, hashtag=False, apostroph=True, slang=False, slang2=False,\n",
    "                         short_word=False, numbers=False, spelling=False, alphabetic=False,\n",
    "                         stopwords=False, stemming=False, lemmatizing=False, lemmatizing2=False, neg_pos_word=False, ekphrasis = True, save_file_name=save_file_name_pos)\n",
    "    \n",
    "    negative = get_pre_process_data(positive= False, full=full, ponctuation=True, letter_repetition=True,\n",
    "                         emoji=False, hashtag=False, apostroph=True, slang=False, slang2=False,\n",
    "                         short_word=False, numbers=False, spelling=False, alphabetic=False,\n",
    "                         stopwords=False, stemming=False, lemmatizing=False, lemmatizing2=False, neg_pos_word=False, ekphrasis = True, save_file_name=save_file_name_neg)\n",
    "    if (testing): \n",
    "        test = get_pre_process_data_test(ponctuation=True, letter_repetition=True,\n",
    "                         emoji=False, hashtag=False, apostroph=True, slang=False, slang2=False,\n",
    "                         short_word=False, numbers=False, spelling=False, alphabetic=False,\n",
    "                         stopwords=False, stemming=False, lemmatizing=False, lemmatizing2=False, neg_pos_word=False, ekphrasis = True, save_file_name=save_file_name_test)\n",
    "    else : \n",
    "        test = []\n",
    "    return positive, negative, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3f3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julie\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\julie\\.ekphrasis\\stats\\twitter\\counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\julie\\.ekphrasis\\stats\\twitter\\counts_2grams.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julie\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'pre_process_doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e2d53252d324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweet_pos_fp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_neg_fp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_test_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesired_preprocessing2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'preprocessing_pos_fp.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'preprocessing_neg_fp.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'preprocessing_test_fp.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-32159b03cea8>\u001b[0m in \u001b[0;36mdesired_preprocessing2\u001b[1;34m(full, testing, save_file_name_pos, save_file_name_neg, save_file_name_test)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#try n°2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdesired_preprocessing2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_file_name_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_file_name_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_file_name_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     positive = get_pre_process_data(positive = True, full=full, ponctuation=True, letter_repetition=True,\n\u001b[0m\u001b[0;32m      4\u001b[0m                          \u001b[0memoji\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapostroph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslang2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                          \u001b[0mshort_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspelling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabetic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\MLProject2\\helper\\pre_processing.py\u001b[0m in \u001b[0;36mget_pre_process_data\u001b[1;34m(positive, full, ponctuation, letter_repetition, emoji, hashtag, apostroph, slang, slang2, short_word, numbers, spelling, alphabetic, stopwords, stemming, lemmatizing, lemmatizing2, neg_pos_word, ekphrasis, save_file_name)\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegative_positive_word_treatment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mekphrasis\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_ekphrasis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_non_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\MLProject2\\helper\\pre_processing.py\u001b[0m in \u001b[0;36mpreprocess_ekphrasis\u001b[1;34m(tweet_list)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_ekphrasis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[0mtext_preprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_text_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_preprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_process_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'pre_process_doc'"
     ]
    }
   ],
   "source": [
    "tweet_pos_fp, tweet_neg_fp, tweet_test_np = desired_preprocessing2(False,True,'preprocessing_pos_fp.txt','preprocessing_neg_fp.txt','preprocessing_test_fp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pos_fp_full, tweet_neg_fp_full, garbage2 = desired_preprocessing2(True,False,'preprocessing_pos_fp_full.txt','preprocessing_neg_fp_full.txt','garbage2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0843687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e8611a",
   "metadata": {},
   "source": [
    "## Prepare for ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(full=False,run_processing = False): \n",
    "    if run_processing : \n",
    "        tweet_pos, tweet_neg, tweet_test = desired_preprocessing(full,'preprocessing_pos.txt','preprocessing_neg.txt','preprocessing_test.txt')\n",
    "    else : \n",
    "        if full : \n",
    "            #change\n",
    "            path_pos = '../Resources/train_pos_processed.txt'\n",
    "            path_neg = '../Resources/train_neg_small_fullprocessed.txt'\n",
    "        else : \n",
    "            path_pos = '../Resources/train_pos_processed.txt'\n",
    "            path_neg = '../Resources/train_neg_small_fullprocessed.txt'\n",
    "        \n",
    "        path_test = '../Resources/test_processed.txt'\n",
    "    \n",
    "        tweet_pos = [tweet.rstrip('\\n') for tweet in open(path_pos)]\n",
    "        tweet_neg = [tweet.rstrip('\\n') for tweet in open(path_neg)]\n",
    "        tweet_test = [tweet.rstrip('\\n') for tweet in open(path_test)]\n",
    "    \n",
    "    data_test = pd.DataFrame({\"tweet\": tweet_test})  \n",
    "    data_pos = pd.DataFrame({\"tweet\": tweet_pos,\"sentiment\": np.ones(len(lines_positive))})\n",
    "    data_neg = pd.DataFrame({ \"tweet\": tweet_neg, \"sentiment\": np.zeros(len(lines_negative)) })\n",
    "    \n",
    "    data_train = pd.concat([data_pos, data_neg],axis=0).reset_index().drop(columns=['index'])\n",
    "\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True).head(1000)\n",
    "\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24fc29",
   "metadata": {},
   "source": [
    "## Run ML algo (ignore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c13e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(clf, X, y):\n",
    "    \"\"\"\n",
    "    Returns the model for clf trained\n",
    "    INPUT:\n",
    "        clf :                     - The classifier to train\n",
    "        X : Multidimensional list - The traning features\n",
    "        y : list                  - The traning results\n",
    "    OUTPUT:\n",
    "        Returns the model trained\n",
    "    \"\"\"\n",
    "    tvec = TfidfVectorizer().set_params(\n",
    "        stop_words=None, max_features=100000, ngram_range=(1, 3))\n",
    "\n",
    "    model_pipeline = Pipeline([('vectorizer', tvec), ('classifier', clf)])\n",
    "    model_pipeline.fit(X, y)\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logistic(data_train):\n",
    "    X = data_train.tweets\n",
    "    y = data_train.sentiments\n",
    "    clf = LogisticRegression()\n",
    "    return train_pipeline(clf, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
