{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import wordninja as wn\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from gingerit.gingerit import GingerIt #pip3 install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/train_neg.txt'\n",
    "PATH_TRAIN_POS = '../Resources/train_pos.txt'\n",
    "PATH_DICT_POS = '../Resources/positive-words.txt'\n",
    "PATH_DICT_NEG = '../Resources/negative-words.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS) as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG) as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "with open(PATH_DICT_POS) as f:\n",
    "    dict_pos = f.read().split()\n",
    "with open(PATH_DICT_NEG) as f:\n",
    "    dict_neg = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ponctuation(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) : \n",
    "        #replace multiple stops by the word 'consecutivestop'\n",
    "        tweet = re.sub(r\"(\\.)\\1+\", ' consecutiveStop ', tweet)\n",
    "        #replace multiple exclamation by the word 'consecutivequestion'\n",
    "        tweet = re.sub(r\"(\\?)\\1+\", ' consecutiveQuestion ', tweet)\n",
    "        #replace multiple exclamation by the word 'consecutiveexclamation'\n",
    "        tweet = re.sub(r\"(\\!)\\1+\", ' consecutiveExclamation ', tweet)\n",
    "        #delete all ponctuaction\n",
    "        tweet = re.sub(r\"[,.;@?!&$\\\\*\\\"]+\\ *\", ' ', tweet)\n",
    "        #deleting consecutive spaces\n",
    "        tweet = re.sub(r\"\\s+\", ' ',tweet)\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_repetition_treatment(tweet_list) : \n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative ', tweet)\n",
    "        # Cry -- :,(, :'(, :\"(\n",
    "        tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative ', tweet)\n",
    "        # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive ', tweet)\n",
    "        # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive ', tweet)\n",
    "        # Love -- <3, :*\n",
    "        tweet = re.sub(r'(<3|:\\*)', ' love ', tweet)\n",
    "        # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "        tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive ', tweet)\n",
    "        tweet_list[i] = tweet\n",
    "\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_set) :\n",
    "        tweet = np.array(tweet.split(),dtype='object')\n",
    "        for word in tweet :\n",
    "            if '#' in word :\n",
    "                index = np.where(tweet == word)\n",
    "                word = \" \".join(wn.split(word))\n",
    "                if (isinstance(tweet, str)):\n",
    "                    tweet.replace('#', '')\n",
    "                else :\n",
    "                    np.put(tweet,index[0][0],word)\n",
    "                tweet = \" \".join(tweet)\n",
    "                tweet_set[i]=tweet\n",
    "            \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we introduce error with \"'s\" ex : my sister's best friend is ... \n",
    "def apostrophe_contraction(tweet_list) :\n",
    "    \n",
    "    contractions = {\n",
    "        '\\'m' : ' am',\n",
    "        'im' : ' I am',\n",
    "        'ive' : 'I have',\n",
    "        '\\'re' : ' are', \n",
    "        '\\'ve' : ' have',\n",
    "        '\\'s' : ' is', \n",
    "        '\\'ll' : ' will',\n",
    "        '\\'d' : ' would', \n",
    "        '\\'t' : ' not',\n",
    "        'ain\\'t' : 'not',\n",
    "        'aint' : 'not',\n",
    "        'can\\'t' : 'can not',\n",
    "        'cant' : 'can not',\n",
    "        'don\\'t' : 'do not',\n",
    "        'dont' : 'do not',\n",
    "        'isn\\'t' : 'is not',\n",
    "        'isnt' : 'is not',\n",
    "        'won\\'t' : 'will not',\n",
    "        'wont' : 'will not',\n",
    "        'shouldn\\'t' : 'should not',\n",
    "        'shouldnt' : 'should not',\n",
    "        'couldn\\'t' : 'could not',\n",
    "        'wouldn\\'t' : 'would not', \n",
    "        'aren\\'t' : 'are not', \n",
    "        'arent' : 'are not', \n",
    "        'doesn\\'t' : 'does not',\n",
    "        'doesnt' : 'does not',\n",
    "        'wasn\\'t' : 'was not',\n",
    "        'wasnt' : 'was not',\n",
    "        'weren\\'t' : 'were not',\n",
    "        'werent' : 'were not',\n",
    "        'hasn\\'t' : 'has not', \n",
    "        'haven\\'t' : 'have not',\n",
    "        'havent' : 'have not',\n",
    "        'hadn\\'t' : 'had not', \n",
    "        'mustn\\'t' : 'must not', \n",
    "        'didn\\'t' : 'did not', \n",
    "        'mightn\\'t' : 'might not', \n",
    "        'needn\\'t' : 'need not',\n",
    "        'imma' : 'i am going to',\n",
    "        'wanna' : 'want to',\n",
    "        'gonna' : 'going to',\n",
    "        'thats' : 'that is',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(contractions))\n",
    "\n",
    "    return [pat.sub(lambda m: contractions.get(m.group()), tweet.lower()) for tweet in tweet_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_slang(tweet_list) : \n",
    "    \n",
    "    slang = {\n",
    "        '2nite' : 'tonight',\n",
    "        '2night' : 'tonight',\n",
    "        '2' : 'to',\n",
    "        '4' : 'for',\n",
    "        'ab' : 'about',\n",
    "        'ace' : 'success',\n",
    "        'ad' : 'awesome person',\n",
    "        'af' : 'very', #mmmh could do better : word af  -> very word maybe ? \n",
    "        'aka' : 'meaning',\n",
    "        'asap' : 'soon',\n",
    "        'aww' : 'cute',\n",
    "        'bc' : 'because',\n",
    "        'bf' : 'boyfriend',\n",
    "        'bff' : 'best friend',\n",
    "        'brb' : 'I come',\n",
    "        'btw' : 'by the way',\n",
    "        'cus' : 'because',\n",
    "        'cuz' : 'because',\n",
    "        'cya' : 'see you',\n",
    "        'dammit' : 'damn it',\n",
    "        'dam' : 'damn',\n",
    "        'der' : 'there',\n",
    "        'dm' : 'message me',\n",
    "        'dunno' : 'do not know',\n",
    "        'dnt' : 'do not',\n",
    "        'dw' : 'okay',\n",
    "        'ew' : 'gross',\n",
    "        'ftw' : 'win',\n",
    "        'fyi' : 'for information',\n",
    "        'gf' : 'girlfriend',\n",
    "        'gotta' : 'has',\n",
    "        'gurl' : 'girl',\n",
    "        'haha' : 'laught',\n",
    "        'hahah' : 'laught',\n",
    "        'hahaha' : 'laught',\n",
    "        'hahahah' : 'laught',\n",
    "        'hahahaha' : 'laught',\n",
    "        'hmu' : 'message me',\n",
    "        'idk' : 'do not know',\n",
    "        'idc' : 'do not care',\n",
    "        'ily' : 'love',\n",
    "        'imo' : 'think',\n",
    "        'irl' : 'real life',\n",
    "        'jk' : 'laught',\n",
    "        'lmao' : 'laught',\n",
    "        'lmk' : 'let me know',\n",
    "        'lil' : 'little',\n",
    "        'lol' : 'laught',\n",
    "        'luv' : 'love',\n",
    "        'ppl' : 'people',\n",
    "        'n' : 'and',\n",
    "        'nbd' : 'okay', #no big deal\n",
    "        'np' : 'okay', #no problem\n",
    "        'nvm' : 'okay', #never mind\n",
    "        'omg' : 'amazing', #oh my god\n",
    "        'omw' : \"come\",\n",
    "        'r' : 'are',\n",
    "        'rofl' : 'laught',\n",
    "        'roflmao' : 'laught',\n",
    "        'rn' : 'now',\n",
    "        'rt' : 'retweet',\n",
    "        'sch' : 'school',\n",
    "        'tbh' : 'honestly',\n",
    "        'til' : 'until',\n",
    "        'thx' : 'thanks',\n",
    "        'ttyl' : 'talk later',\n",
    "        'u' : 'you',\n",
    "        'ur' : 'your',\n",
    "        'w' : 'with',\n",
    "        'wan' : 'want',\n",
    "        'waz' : 'what is',\n",
    "        'wtf' : 'seriously',\n",
    "        'x' : 'kiss',\n",
    "        'xx' : 'kiss',\n",
    "        'xo' : 'kiss',\n",
    "        'xoxo' : 'kiss',\n",
    "        'xd' : 'laught',\n",
    "        'y' : 'why',\n",
    "        'ya' : 'you',\n",
    "        'yay' : 'happy',\n",
    "        'yolo' : 'enjoy',\n",
    "        'yuck' : 'gross',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(slang))\n",
    "\n",
    "    return [pat.sub(lambda m: slang.get(m.group()), tweet.lower()) for tweet in tweet_list]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_slang2(tweet_list) : \n",
    "    \n",
    "    parser = GingerIt()\n",
    "    for i, tweet in enumerate(tweet_list) : \n",
    "        t = parser.parse(tweet)\n",
    "        tweet_list[index] = t.get('result')\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_word_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = \" \".join([word for word in tweet.split() if len(word) > 1])\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marche pas \n",
    "def numbers_treatment(tweet):\n",
    "    \n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            word = re.sub('[,\\.:%_\\-\\+\\*\\/\\%\\_]', '', word)\n",
    "            float(word)\n",
    "            new_tweet.append(\"\")\n",
    "        except:\n",
    "            new_tweet.append(word)\n",
    "            \n",
    "    return \" \".join(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 5 5 , 4ever ,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_treatment('4 5 5 , 4ever , ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(tweet_list):\n",
    "    \n",
    "    sp = SpellCorrector(corpus=\"english\")\n",
    "    \n",
    "    return [sp.correct_text(tweet) for tweet in tweet_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IN am as boy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SpellCorrector(corpus=\"english\")\n",
    "sp.correct_text('I am a boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_alphabetic_treatment(tweet_list) : \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = \" \".join([word for word in tweet.split() if word.isalpha()])\n",
    "        tweet_list[i] = tweet\n",
    "\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I am go to read my document']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['I am going to read my documentation']\n",
    "\n",
    "def stemming_treatment(tweet_list):\n",
    "    for line in range(len(tweet_list)) :\n",
    "        print(type(line))\n",
    "        print(line)\n",
    "        tweet = tweet_list[line]\n",
    "        tweet =  \" \".join(stemmer.stem(t) for t in tweet.split())\n",
    "        tweet_list[line] = tweet\n",
    "        \n",
    "    return tweet_list\n",
    "stemming_treatment(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = replace_ponctuation(train_pos)\n",
    "train_neg = replace_ponctuation(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = letter_repetition_treatment(train_pos)\n",
    "train_neg = letter_repetition_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = emoji_treatment(train_pos)\n",
    "train_neg = emoji_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = hashtag_treatment(train_pos)\n",
    "train_neg = hashtag_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = apostrophe_contraction(train_pos)\n",
    "train_neg = apostrophe_contraction(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = correct_slang(train_pos)\n",
    "train_neg = correct_slang(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = correct_slang2(train_pos)\n",
    "#train_neg = correct_slang2(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = short_word_treatment(train_pos)\n",
    "train_neg = short_word_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = numbers_treatment(train_pos)\n",
    "train_neg = numbers_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = correct_spelling(train_pos)\n",
    "#train_neg = correct_spelling(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = non_alphabetic_treatment(train_pos)\n",
    "train_neg = non_alphabetic_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = stemming_treatment(train_pos)\n",
    "train_neg = stemming_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(train_pos,train_neg):\n",
    "    \n",
    "    train_pos = np.array(train_pos).reshape(-1,1)\n",
    "    ones = np.ones(shape=(train_pos.shape[0],1))\n",
    "    train_pos = np.concatenate((train_pos,ones),axis = 1)\n",
    "\n",
    "    train_neg = np.array(train_neg).reshape(-1,1)\n",
    "    neg_ones = np.zeros(shape=(train_neg.shape[0],1))-1\n",
    "    train_neg = np.concatenate((train_neg,neg_ones),axis = 1)\n",
    "    \n",
    "    return (train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos,train_neg = label_data(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of \\<user> and \\<url> on the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tag_impact(train_pos,train_neg):\n",
    "    \n",
    "    user = \"<user>\"\n",
    "    user_count_pos = 0\n",
    "    user_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if user in train_pos[i] :\n",
    "            user_count += 1\n",
    "            user_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if user in train_neg[i] :\n",
    "            user_count += 1\n",
    "            \n",
    "    user_count_neg = user_count - user_count_pos\n",
    "    counts = np.array([user_count,user_count_pos,user_count_neg])\n",
    "\n",
    "    user_dict = {\"Positive Sentiment Tweet\":user_count_pos,\"Negative Sentiment Tweet\":user_count_neg}\n",
    "    keys = list(user_dict.keys())\n",
    "    vals = [user_dict[k] for k in keys]\n",
    "    ax1 = sns.barplot(x=keys, y=vals)   \n",
    "    ax1.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax1.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax1.set_title(\"User Tag Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tag_counts = url_impact(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_impact(train_pos,train_neg):\n",
    "    \n",
    "    url = \"<url>\"\n",
    "    url_count_pos = 0\n",
    "    url_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if url in train_pos[i] :\n",
    "            url_count += 1\n",
    "            url_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if url in train_neg[i] :\n",
    "            url_count += 1\n",
    "            \n",
    "    url_count_neg = url_count - url_count_pos\n",
    "    counts = np.array([url_count,url_count_pos,url_count_neg])\n",
    "    \n",
    "    url_dict = {\"Positive Sentiment Tweet\":url_count_pos,\"Negative Sentiment Tweet\":url_count_neg}\n",
    "    keys = list(url_dict.keys())\n",
    "    vals = [url_dict[k] for k in keys]\n",
    "    ax = sns.barplot(x=keys, y=vals)   \n",
    "    ax.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax.set_title(\"Url Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag_counts = user_tag_impact(train_pos,train_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
