{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import wordninja as wn\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from gingerit.gingerit import GingerIt #pip3 install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/train_neg.txt'\n",
    "PATH_TRAIN_POS = '../Resources/train_pos.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS) as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG) as f:\n",
    "    train_neg = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ponctuation(tweet):\n",
    "    \n",
    "    #replace multiple stops by the word 'consecutivestop'\n",
    "    tweet = re.sub(r\"(\\.)\\1+\", ' consecutiveStop ', tweet)\n",
    "    #replace multiple exclamation by the word 'consecutivequestion'\n",
    "    tweet = re.sub(r\"(\\?)\\1+\", ' consecutiveQuestion ', tweet)\n",
    "    #replace multiple exclamation by the word 'consecutiveexclamation'\n",
    "    tweet = re.sub(r\"(\\!)\\1+\", ' consecutiveExclamation ', tweet)\n",
    "    #delete all ponctuaction\n",
    "    tweet = re.sub(r\"[,.;@?!#&$\\\"]+\\ *\", ' ', tweet)\n",
    "    #deleting consecutive spaces\n",
    "    tweet = re.sub(r\"\\s+\", ' ',tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_repetition_treatment(tweet) : \n",
    "    \n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_treatment(tweet):\n",
    "    \n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' love ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive ', tweet)\n",
    "\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : correct behaviour \n",
    "def hashtag_treatment(tweet_set):\n",
    "    \n",
    "    for line in range(len(tweet_set)) :\n",
    "        tweet = tweet_set[line]\n",
    "        tweet = np.array(tweet.split())\n",
    "        for word in tweet :\n",
    "            if '#' in word :\n",
    "                index = np.where(tweet ==word)\n",
    "                word = \" \".join(wn.split(word))\n",
    "                if (isinstance(tweet, str)):\n",
    "                    tweet.replace('#', '')\n",
    "                else :\n",
    "                    tweet[index] = word\n",
    "                tweet = \" \".join(tweet)\n",
    "                tweet_set[line]=tweet\n",
    "            \n",
    "    return tweet_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ive got new', 'wtf is going on Machine Learning Driving Me Cr']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_treatment(['#ivegotnews','wtf is going on #MachineLearningDrivingMeCrazy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we introduce error with \"'s\" ex : my sister's best friend is ... \n",
    "def apostrophe_contraction(tweet_list) :\n",
    "    \n",
    "    contractions = {\n",
    "        '\\'m' : ' am',\n",
    "        'im' : ' I am',\n",
    "        'ive' : 'I have',\n",
    "        '\\'re' : ' are', \n",
    "        '\\'ve' : ' have',\n",
    "        '\\'s' : ' is', \n",
    "        '\\'ll' : ' will',\n",
    "        '\\'d' : ' would', \n",
    "        '\\'t' : ' not',\n",
    "        'ain\\'t' : 'not',\n",
    "        'aint' : 'not',\n",
    "        'can\\'t' : 'can not',\n",
    "        'cant' : 'can not',\n",
    "        'don\\'t' : 'do not',\n",
    "        'dont' : 'do not',\n",
    "        'isn\\'t' : 'is not',\n",
    "        'isnt' : 'is not',\n",
    "        'won\\'t' : 'will not',\n",
    "        'wont' : 'will not',\n",
    "        'shouldn\\'t' : 'should not',\n",
    "        'shouldnt' : 'should not',\n",
    "        'couldn\\'t' : 'could not',\n",
    "        'wouldn\\'t' : 'would not', \n",
    "        'aren\\'t' : 'are not', \n",
    "        'arent' : 'are not', \n",
    "        'doesn\\'t' : 'does not',\n",
    "        'doesnt' : 'does not',\n",
    "        'wasn\\'t' : 'was not',\n",
    "        'wasnt' : 'was not',\n",
    "        'weren\\'t' : 'were not',\n",
    "        'werent' : 'were not',\n",
    "        'hasn\\'t' : 'has not', \n",
    "        'haven\\'t' : 'have not',\n",
    "        'havent' : 'have not',\n",
    "        'hadn\\'t' : 'had not', \n",
    "        'mustn\\'t' : 'must not', \n",
    "        'didn\\'t' : 'did not', \n",
    "        'mightn\\'t' : 'might not', \n",
    "        'needn\\'t' : 'need not',\n",
    "        'imma' : 'I am going to',\n",
    "        'wanna' : 'want to',\n",
    "        'gonna' : 'going to',\n",
    "        'thats' : 'that is',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(contractions))\n",
    "\n",
    "    return [pat.sub(lambda m: contractions.get(m.group()), tweet.lower()) for tweet in tweet_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_slang(tweet_list) : \n",
    "    slang = {\n",
    "        '2nite' : 'tonight',\n",
    "        '2night' : 'tonight',\n",
    "        '2' : 'to',\n",
    "        '4' : 'for',\n",
    "        '' : '',\n",
    "        'ab' : 'about',\n",
    "        'ace' : 'success',\n",
    "        'ad' : 'awesome person',\n",
    "        'aka' : 'meaning',\n",
    "        'asap' : 'soon',\n",
    "        'aww' : 'cute',\n",
    "        'bc' : 'because',\n",
    "        'bf' : 'boyfriend',\n",
    "        'bff' : 'best friend',\n",
    "        'brb' : 'I come',\n",
    "        'btw' : 'by the way',\n",
    "        'cus' : 'because',\n",
    "        'cuz' : 'because',\n",
    "        'cya' : 'see you',\n",
    "        'dammit' : 'damn it',\n",
    "        'der' : 'there',\n",
    "        'dm' : 'message me',\n",
    "        'dunno' : 'do not know',\n",
    "        'dw' : 'okay',\n",
    "        'ew' : 'gross',\n",
    "        'ftw' : 'win',\n",
    "        'fyi' : 'for information',\n",
    "        'gf' : 'girlfriend',\n",
    "        'gotta' : 'has',\n",
    "        'gurl' : 'girl',\n",
    "        'haha' : 'laught',\n",
    "        'hahah' : 'laught',\n",
    "        'hahaha' : 'laught',\n",
    "        'hahahah' : 'laught',\n",
    "        'hahahaha' : 'laught',\n",
    "        'hmu' : 'message me',\n",
    "        'idk' : 'do not know',\n",
    "        'idc' : 'do not care',\n",
    "        'ily' : 'love',\n",
    "        'imo' : 'think',\n",
    "        'irl' : 'real life',\n",
    "        'jk' : 'laught',\n",
    "        'lmao' : 'laught',\n",
    "        'lmk' : 'let me know',\n",
    "        'lil' : 'little',\n",
    "        'lol' : 'laught',\n",
    "        'luv' : 'love',\n",
    "        'ppl' : 'people',\n",
    "        'n' : 'and',\n",
    "        'nbd' : 'okay', #no big deal\n",
    "        'np' : 'okay', #no problem\n",
    "        'nvm' : 'okay', #never mind\n",
    "        'omg' : 'amazing', #oh my god\n",
    "        'omw' : \"come\",\n",
    "        'r' : 'are',\n",
    "        'rofl' : 'laught',\n",
    "        'roflmao' : 'laught',\n",
    "        'rn' : 'now',\n",
    "        'rt' : 'retweet',\n",
    "        'sch' : 'school',\n",
    "        'tbh' : 'honestly',\n",
    "        'til' : 'until',\n",
    "        'thx' : 'thanks',\n",
    "        'ttyl' : 'talk later',\n",
    "        'u' : 'you',\n",
    "        'ur' : 'your',\n",
    "        'w' : 'with',\n",
    "        'wan' : 'want',\n",
    "        'waz' : 'what is',\n",
    "        'wtf' : 'seriously',\n",
    "        'x' : 'kiss',\n",
    "        'xx' : 'kiss',\n",
    "        'xo' : 'kiss',\n",
    "        'xoxo' : 'kiss',\n",
    "        'xd' : 'laught',\n",
    "        'ya' : 'you',\n",
    "        'yolo' : 'enjoy',\n",
    "        'yuck' : 'gross',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(slang))\n",
    "\n",
    "    return [pat.sub(lambda m: slang.get(m.group()), tweet.lower()) for tweet in tweet_list]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_slang2(tweet_list) : \n",
    "    \n",
    "    parser = GingerIt()\n",
    "    for index in range(len(tweet_list)) : \n",
    "        tweet = tweet_list[index]\n",
    "        t = parser.parse(tweet)\n",
    "        tweet_list[index] = t.get('result')\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_word_treatment(tweet):\n",
    "    \n",
    "    return \" \".join([word for word in tweet.split() if len(word) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_treatment(tweet):\n",
    "    \n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            word = re.sub('[,\\.:%_\\-\\+\\*\\/\\%\\_]', '', word)\n",
    "            float(word)\n",
    "            new_tweet.append(\"\")\n",
    "        except:\n",
    "            new_tweet.append(word)\n",
    "            \n",
    "    return \" \".join(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(tweet_list):\n",
    "    \n",
    "    sp = SpellCorrector(corpus=\"english\")\n",
    "    \n",
    "    return [sp.correct_text(tweet) for tweet in tweet_list]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_alphabetic_treatment(tweet) : \n",
    "\n",
    "    return \" \".join([word for word in tweet.split() if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [replace_ponctuation(tweet) for tweet in train_pos]\n",
    "train_neg = [replace_ponctuation(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [letter_repetition_treatment(tweet) for tweet in train_pos]\n",
    "train_neg = [letter_repetition_treatment(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [emoji_treatment(tweet) for tweet in train_pos]\n",
    "train_neg = [emoji_treatment(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = hashtag_treatment(train_pos)\n",
    "train_neg = hashtag_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = apostrophe_contraction(train_pos)\n",
    "train_neg = apostrophe_contraction(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = correct_slang(train_pos)\n",
    "train_neg = correct_slang(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = correct_slang2(train_pos)\n",
    "train_neg = correct_slang2(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [short_word_treatment(tweet) for tweet in train_pos]\n",
    "train_neg = [short_word_treatment(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [numbers_treatment(tweet) for tweet in train_pos]\n",
    "train_neg = [numbers_treatment(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = correct_spelling(train_pos)\n",
    "#train_neg = correct_spelling(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SpellCorrector(corpus=\"english\")\n",
    "sp.correct_text(\"this is a text, withh some mistakds!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = [non_alphabetic_treatment(tweet) for tweet in train_pos]\n",
    "train_neg = [non_alphabetic_treatment(tweet) for tweet in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(train_pos,train_neg):\n",
    "    \n",
    "    train_pos = np.array(train_pos).reshape(-1,1)\n",
    "    ones = np.ones(shape=(train_pos.shape[0],1))\n",
    "    train_pos = np.concatenate((train_pos,ones),axis = 1)\n",
    "\n",
    "    train_neg = np.array(train_neg).reshape(-1,1)\n",
    "    neg_ones = np.zeros(shape=(train_neg.shape[0],1))-1\n",
    "    train_neg = np.concatenate((train_neg,neg_ones),axis = 1)\n",
    "    \n",
    "    return (train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos,train_neg = label_data(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of \\<user> and \\<url> on the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tag_impact(train_pos,train_neg):\n",
    "    \n",
    "    user = \"<user>\"\n",
    "    user_count_pos = 0\n",
    "    user_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if user in train_pos[i] :\n",
    "            user_count += 1\n",
    "            user_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if user in train_neg[i] :\n",
    "            user_count += 1\n",
    "            \n",
    "    user_count_neg = user_count - user_count_pos\n",
    "    counts = np.array([user_count,user_count_pos,user_count_neg])\n",
    "\n",
    "    user_dict = {\"Positive Sentiment Tweet\":user_count_pos,\"Negative Sentiment Tweet\":user_count_neg}\n",
    "    keys = list(user_dict.keys())\n",
    "    vals = [user_dict[k] for k in keys]\n",
    "    ax1 = sns.barplot(x=keys, y=vals)   \n",
    "    ax1.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax1.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax1.set_title(\"User Tag Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tag_counts = url_impact(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_impact(train_pos,train_neg):\n",
    "    \n",
    "    url = \"<url>\"\n",
    "    url_count_pos = 0\n",
    "    url_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if url in train_pos[i] :\n",
    "            url_count += 1\n",
    "            url_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if url in train_neg[i] :\n",
    "            url_count += 1\n",
    "            \n",
    "    url_count_neg = url_count - url_count_pos\n",
    "    counts = np.array([url_count,url_count_pos,url_count_neg])\n",
    "    \n",
    "    url_dict = {\"Positive Sentiment Tweet\":url_count_pos,\"Negative Sentiment Tweet\":url_count_neg}\n",
    "    keys = list(url_dict.keys())\n",
    "    vals = [url_dict[k] for k in keys]\n",
    "    ax = sns.barplot(x=keys, y=vals)   \n",
    "    ax.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax.set_title(\"Url Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag_counts = user_tag_impact(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
