{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2151da",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669d86a",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a292d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.models import Word2Vec                                   #pip install gensim\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import gensim.downloader as api\n",
    "from glove import Corpus, Glove                                      #pip install glove-py\n",
    "from joblib import dump, load\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "407f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/train_neg_processed.txt'\n",
    "PATH_TRAIN_POS = '../Resources/train_pos_processed.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS) as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG) as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "\n",
    "train_set = train_pos + train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f302c3",
   "metadata": {},
   "source": [
    "## Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4dda91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"basic and naive method for word embedding\"\n",
    "def we_count_vectorize(train_set) :\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    text_counts = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "    \n",
    "    return voc, text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65ffb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Sparse matrix : would require way too much space (around 38GB)\"\n",
    "voc, text_counts = we_count_vectorize(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e8f78",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6d7a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TF-IDF\n",
      "jaybe                  0.0\n",
      "jaycee                 0.0\n",
      "jayesslee              0.0\n",
      "jawa                   0.0\n",
      "jayhawk                0.0\n",
      "jaylin                 0.0\n",
      "jaylor                 0.0\n",
      "jaypark                0.0\n",
      "jazbat                 0.0\n",
      "jbl                    0.0\n",
      "jbos                   0.0\n",
      "jcksnxkwnxlwncowkd     0.0\n",
      "jawaad                 0.0\n",
      "javosync               0.0\n",
      "jasjam                 0.0\n",
      "jauh                   0.0\n",
      "jaskknc                0.0\n",
      "jasminator             0.0\n",
      "jassell                0.0\n",
      "jata                   0.0\n",
      "jati                   0.0\n",
      "jatoba                 0.0\n",
      "jatt                   0.0\n",
      "jatuh                  0.0\n",
      "jaunty                 0.0\n",
      "javon                  0.0\n",
      "javafx                 0.0\n",
      "javanese               0.0\n",
      "javascr                0.0\n",
      "javascript             0.0\n",
      "javelin                0.0\n",
      "javi                   0.0\n",
      "javier                 0.0\n",
      "javoedge               0.0\n",
      "zzcase                 0.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfIdf = vectorizer.fit_transform(train_set)\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.tail(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fad54053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_idf(X):\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(X)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23e20f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-9e171933a301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapply_tf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-189f820f32be>\u001b[0m in \u001b[0;36mapply_tf_idf\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mapply_tf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "apply_tf_idf(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25194ad",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, positive_corpus,negative_corpus):\n",
    "        \n",
    "        self.positive_corpus = positive_corpus\n",
    "        self.negative_corpus = negative_corpus\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in open(self.positive_corpus):            \n",
    "            yield utils.simple_preprocess(line)\n",
    "        for line in open(self.negative_corpus):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d185f3",
   "metadata": {},
   "source": [
    "#### do not run this cell again : run the next one to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neural embedding model\n",
    "\"\"\"\n",
    "def we_word2vector_create_model(path_train_pos, path_train_neg) :\n",
    "    \n",
    "    corpus = MyCorpus(path_train_pos, path_train_neg)\n",
    "    model = Word2Vec(sentences = corpus)\n",
    "    model.save('word2vec_saved_model.joblib')\n",
    "    model.wv.save(\"word2vec.wordvectors\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3b420",
   "metadata": {},
   "source": [
    "#### load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path_train_pos = ..\\\\Resources\\\\train_pos.txt\n",
    "# for path_train_heg = ..\\\\Resources\\\\train_neg.txt\n",
    "def we_word2vector_load_model() :\n",
    "    return gensim.models.Word2Vec.load('word2vec_saved_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b2f2f",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95415523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Glove: Global Vectors for Word Representation\"\n",
    "\n",
    "def we_glove(train_set) : \n",
    "    # model = list of sentences\n",
    "    model = api.load(\"glove-twitter-25\")\n",
    "    #corpus = 1 list of words\n",
    "    corpus = list(itertools.chain.from_iterable(model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ea872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
