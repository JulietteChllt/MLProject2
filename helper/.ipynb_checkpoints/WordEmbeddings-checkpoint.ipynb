{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b957f9e",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669d86a",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a292d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import necessary libraries\n",
    "\"\"\"\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import keras                                                            #pip install keras #pip install tensorflow\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Conv1D, Dropout, Flatten, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import Word2Vec                                   #pip install gensim\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim import utils\n",
    "from glove import Corpus, Glove                                      #pip install glove-python-binary \n",
    "from joblib import dump, load\n",
    "from pre_processing import get_pre_process_data_test\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "407f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load full train data processed and build train set \n",
    "\"\"\"\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_fp_full.txt'\n",
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_fp_full.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS,errors = 'ignore') as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG,errors = 'ignore') as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "\n",
    "train_set = train_pos + train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f302c3",
   "metadata": {},
   "source": [
    "## Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dda91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basic and naive method for word embedding\n",
    "\"\"\"\n",
    "def we_count_vectorize(train_set) :\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    text_counts = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "    \n",
    "    #df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"count vectorize\"])\n",
    "    #df = df.sort_values('Count frequency', ascending=False)\n",
    "    #print(df.head(25))\n",
    "    \n",
    "    return voc, text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ffb8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sparse matrix : would require way too much space (around 38GB)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Sparse matrix : would require way too much space (around 38GB)\"\n",
    "voc, text_counts = we_count_vectorize(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a532f5",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41190214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUT : \n",
    "    train_set : list         - contains all the positive and negative tweets\n",
    "    n_min : int              - the minimal number or words in the word representation of the vocabulary (set to one)\n",
    "    n_max : int              - maximal number of words : chose 3 for 3-gram, 4 for 4-gram...\n",
    "\"\"\"\n",
    "def we_tfIdf(train_set,n_min,n_max) : \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(n_min, n_max))\n",
    "    text_tfIdf = vectorizer.fit_transform(train_set)\n",
    "    voc = vectorizer.vocabulary_\n",
    "\n",
    "    #df = pd.DataFrame(text_tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    #df = df.sort_values('TF-IDF', ascending=False)\n",
    "    #print (df.head(25))\n",
    "    \n",
    "    return voc, text_tfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890ffba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_tf, tf_idf = we_tfIdf(train_set,1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25194ad",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42e03c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Word2vector corpus \n",
    "\"\"\"\n",
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, positive_corpus,negative_corpus):\n",
    "        \n",
    "        self.positive_corpus = positive_corpus\n",
    "        self.negative_corpus = negative_corpus\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in open(self.positive_corpus):            \n",
    "            yield utils.simple_preprocess(line)\n",
    "        for line in open(self.negative_corpus):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f686d",
   "metadata": {},
   "source": [
    "#### do not run this cell again : run the next one to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neural embedding model : saves and return Word2vector model\n",
    "    INPUT : \n",
    "        path_train_pos: string     \n",
    "        path_train_neg: string\n",
    "    OUTPUT : \n",
    "        returns the Word2vector model on the corpus \n",
    "\"\"\"\n",
    "def we_word2vector_create_model(path_train_pos, path_train_neg) :\n",
    "    \n",
    "    corpus = MyCorpus(path_train_pos, path_train_neg)\n",
    "    model = Word2Vec(sentences = corpus)\n",
    "    model.save('word2vec_saved_model.joblib')\n",
    "    model.wv.save(\"word2vec.wordvectors\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2649693",
   "metadata": {},
   "source": [
    "#### load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f2a50290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load the Word2Vector model previously saved\n",
    "\"\"\"\n",
    "# for path_train_pos = ..\\\\Resources\\\\train_pos.txt\n",
    "# for path_train_heg = ..\\\\Resources\\\\train_neg.txt\n",
    "def we_word2vector_load_model() :\n",
    "    return gensim.models.Word2Vec.load('word2vec_saved_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4030853",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99c53a",
   "metadata": {},
   "source": [
    "### load 2nd best processing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59aff8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_full.txt'\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_full.txt'\n",
    "\n",
    "# Load the preprocessed datasets already computed\n",
    "\n",
    "def get_input() :\n",
    "    with open(PATH_TRAIN_POS) as f:\n",
    "        train_pos = f.read().splitlines()\n",
    "    with open(PATH_TRAIN_NEG) as f:\n",
    "        train_neg = f.read().splitlines()\n",
    "\n",
    "    train_set = train_pos + train_neg\n",
    "\n",
    "    y = np.array(len(train_pos) * [1] + len(train_neg) * [0])\n",
    "\n",
    "    test_set = get_pre_process_data_test(save_file_name='test_data_process.txt')\n",
    "\n",
    "    return train_set, y, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2545d",
   "metadata": {},
   "source": [
    "### load 1st best processing files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4aa26e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_NEG = '../Resources/preprocessing_neg_fp_full.txt'\n",
    "PATH_TRAIN_POS = '../Resources/preprocessing_pos_fp_full.txt'\n",
    "\n",
    "# Load the preprocessed datasets already computed\n",
    "\n",
    "def get_input_fp() :\n",
    "    with open(PATH_TRAIN_POS) as f:\n",
    "        train_pos = f.read().splitlines()\n",
    "    with open(PATH_TRAIN_NEG) as f:\n",
    "        train_neg = f.read().splitlines()\n",
    "\n",
    "    train_set = train_pos + train_neg\n",
    "\n",
    "    y = np.array(len(train_pos) * [1] + len(train_neg) * [0])\n",
    "\n",
    "    test_set = get_pre_process_data_test(save_file_name='preprocessing_test_fp.txt')\n",
    "\n",
    "    return train_set, y, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d81f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create glove corpus from training set \n",
    "\"\"\"\n",
    "def create_corpus(train_set):\n",
    "    \n",
    "    corpus=[]\n",
    "    for tweet in train_set:\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if(len(tweet)>1 and word.isalpha())]\n",
    "        corpus.append(words)\n",
    "        corpus = [element for element in corpus if len(element)>0]\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dcfb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create embedding dictionary from GloVe pretrained twitter dataset\n",
    "here we use the one of dimension 50\n",
    "    OUTPUTS :\n",
    "         The embedded matrix from the glove database\n",
    "\"\"\"\n",
    "def create_pretrained() : \n",
    "    \n",
    "    vocabulary_size = 20000\n",
    "    embedding_dict = dict()\n",
    "    with open('../Resources/twitter_dict/glove.twitter.27B.50d.txt','rb') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word.decode()] = coefs\n",
    "    f.close()    \n",
    "    return embedding_dict, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c5bcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create our embedded matrix from our train dataset\n",
    "    INPUTS : \n",
    "        train_set : list of tweets\n",
    "    OUTPUTS :\n",
    "       The embedded matrix made of our train set\n",
    "\"\"\"\n",
    "def create_glove_emb(train_set) :\n",
    "    \n",
    "    model = Corpus()\n",
    "    train_splitted = [tweet.split() for tweet in train_set]\n",
    "    model.fit(train_splitted, window = 5)\n",
    "    \n",
    "    glove = Glove(no_components=50, learning_rate=0.05)\n",
    "    glove.fit(model.matrix, epochs=50)\n",
    "    glove.add_dictionary(model.dictionary)\n",
    "    glove.save('glove.model')\n",
    "    \n",
    "    embedding_dict = {}\n",
    "    for w, id_ in glove.dictionary.items():\n",
    "        embedding_dict[w] = np.array(glove.word_vectors[id_])\n",
    "\n",
    "    return embedding_dict, len(train_set) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61ab9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    INPUTS :\n",
    "        train_set : list of tweets\n",
    "        use_pretrained : True to use the glove pre trained\n",
    "    OUTPUTS :\n",
    "        the embedded dictionary\n",
    "\"\"\"\n",
    "def load_embedding_dict(train_set, use_pretrained=True):\n",
    "\n",
    "    if (use_pretrained) : \n",
    "        return create_pretrained()\n",
    "    else : \n",
    "        return create_glove_emb(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dbc9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert the words of the tweets to vector, they will be used in this form during the training of the model\n",
    "     INPUTS :\n",
    "        train_set : list of tweets\n",
    "        y : sentiments (positive or negative) of the train_set\n",
    "        vector_size : dimension of the vector words\n",
    "    OUTPUTS :\n",
    "        The train_set in a vector form and shuffled\n",
    "\"\"\"\n",
    "def create_sequence(train_set, y, vector_size, tokenizer) : \n",
    "   \n",
    "    tokenizer.fit_on_texts(train_set)\n",
    "    sequences = tokenizer.texts_to_sequences(train_set)\n",
    "    \n",
    "    tweet_pad = pad_sequences(sequences,maxlen=vector_size,truncating='post',padding='post')\n",
    "\n",
    "    indices = np.arange(tweet_pad.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    tweet_pad = tweet_pad[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    return tweet_pad , y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aed24e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the embedded matrix that will be used to make the model\n",
    "    INPUTS :\n",
    "        train_set : list of tweets\n",
    "        y : sentiments (positive or negative) of the train_set\n",
    "            vector_size : dimension of the vector words\n",
    "    OUTPUTS :\n",
    "        returns the vectorized train_set, the embedded matrix, \n",
    "            and the sentiment shuffled accordingly\n",
    "\"\"\"\n",
    "def we_glove(train_set, y,use_pretrained, vector_size):\n",
    "   \n",
    "    embedding_dict, vocabulary_size = load_embedding_dict(train_set, use_pretrained)\n",
    "    tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "    sequence,y = create_sequence(train_set,y, vector_size, tokenizer)\n",
    "\n",
    "    embedding_matrix = np.zeros((vocabulary_size, vector_size))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = embedding_dict.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "            \n",
    "    return sequence, embedding_matrix, y, vocabulary_size, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120d80c",
   "metadata": {},
   "source": [
    "### Test both pretrained and in house glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e1ccc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, glove_matrix, y, vocabuliary_size, tokenizer  = we_glove(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8df4084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1, glove_matrix1, embedding_dict1, nb_words1, y1  = we_glove(train_set,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "92c4c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_word(embedding_dict, emmbedes):\n",
    "    nearest = sorted(embedding_dict.keys(), key=lambda word: spatial.distance.euclidean(embedding_dict[word], emmbedes))\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "de0c639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'you',\n",
       " 'much',\n",
       " 'always',\n",
       " 'know',\n",
       " 'loves',\n",
       " 'miss',\n",
       " 'loving',\n",
       " 'true',\n",
       " 'life']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(embedding_dict, embedding_dict['love'])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9fce2686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'kiss\",\"positive',\n",
       " 'fan',\n",
       " 'beautiful',\n",
       " 'know\",\"personally',\n",
       " 'love\",\"positive',\n",
       " 'rain\",\"positive',\n",
       " 'justin\",\"wear',\n",
       " 'kt\",\"really',\n",
       " 'youu']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(embedding_dict1, embedding_dict1['love'])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c002c24",
   "metadata": {},
   "source": [
    "### train on glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b8591aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, vocabulary_size, vector_dimension) :\n",
    "    '''\n",
    "        compute the model using 3 hidden layers and a sigmoid activation\n",
    "\n",
    "        INPUTS : \n",
    "            embedding_matrix : the glove embedded matrix of size vocabulary_size * vector_dimension\n",
    "        OUTPUTS :\n",
    "            The model ready to be trained\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            vocabulary_size,\n",
    "            vector_dimension,\n",
    "            input_length=50,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False))\n",
    "    model.add(\n",
    "        Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(\n",
    "        Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(\n",
    "        Conv1D(filters=32, kernel_size=7, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(\n",
    "        Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adam', metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f966673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y) :\n",
    "    '''\n",
    "        Train the model with the sentiments of the train_set\n",
    "        INPUTS :\n",
    "            model : model to be trained\n",
    "            X : the vectorize form of the train set\n",
    "            y : the sentiment of each tweet in X\n",
    "\n",
    "        OUTPUTS :\n",
    "            the trained model\n",
    "    '''\n",
    "    model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        batch_size=200,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        epochs=5,\n",
    "        callbacks=[\n",
    "                ModelCheckpoint(\n",
    "                filepath='Embeddings_best_weights.hdf5',\n",
    "                monitor='val_acc',\n",
    "                verbose=1,\n",
    "                save_best_only=True,\n",
    "                mode='max'),\n",
    "                EarlyStopping(\n",
    "                    monitor='val_acc', patience=3, mode='max')\n",
    "                ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ad3611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test, tokenizer):\n",
    "    '''\n",
    "        Put the test tweets in vector forms and predict them with the model\n",
    "        OUTPUTS :\n",
    "            the predictions, each predictions is in the range [0,1]\n",
    "    '''\n",
    "    test_sequences = tokenizer.texts_to_sequences(test)\n",
    "    test = pad_sequences(test_sequences, maxlen=50)\n",
    "    return model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef4f514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(predictions) :\n",
    "    '''\n",
    "        write the predictions in the glove_results file\n",
    "        INPUTS :\n",
    "            prediction : 10 000 sentiments of the test tweets in range [0,1]\n",
    "    '''\n",
    "    predictions =list(zip(range(1, 10001),predictions))\n",
    "    with open('../Resources/glove_result.csv', 'w') as out:\n",
    "        writer = csv.writer(out)\n",
    "        writer.writerow([\"Id\", \"Prediction\"])\n",
    "        for a,b in predictions:\n",
    "            if b < 0.5:\n",
    "                writer.writerow([a, -1])\n",
    "            else:\n",
    "                writer.writerow([a, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "00165563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_glove(vector_size = 50, use_pretrained=True) :\n",
    "\n",
    "    # load the pre processed input : 2nd best preprocessing\n",
    "    X, y, test = get_input()\n",
    "    \n",
    "    # load the pre processed input :  best preprocessing\n",
    "    #X, y, test = get_input_fp()\n",
    "\n",
    "    # compute the embedded matrix\n",
    "    sequence, glove_matrix, y, vocabulary_size, tokenizer  = we_glove(X,y,use_pretrained, vector_size)\n",
    "\n",
    "    # create a model and train it with our train dataset\n",
    "    model = get_model(glove_matrix, vocabulary_size,vector_size)\n",
    "    model_trained = train_model(model,sequence,y)\n",
    "\n",
    "    # make the predictions of our test dataset with our model\n",
    "    predictions = make_predictions(model_trained, test, tokenizer)\n",
    "    make_submission(predictions)\n",
    "    return model_trained, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f626b9f",
   "metadata": {},
   "source": [
    "## Run models with different preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e381c6",
   "metadata": {},
   "source": [
    "### 2nd best preprocessing with pretrained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5086695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 50, 50)            1000000   \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (None, 50, 128)           32128     \n",
      "                                                                 \n",
      " max_pooling1d_24 (MaxPoolin  (None, 25, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 25, 128)           0         \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (None, 25, 64)            49216     \n",
      "                                                                 \n",
      " max_pooling1d_25 (MaxPoolin  (None, 12, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 12, 32)            14368     \n",
      "                                                                 \n",
      " max_pooling1d_26 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " conv1d_27 (Conv1D)          (None, 12, 32)            8224      \n",
      "                                                                 \n",
      " max_pooling1d_27 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 385       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,104,321\n",
      "Trainable params: 104,321\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.8027\n",
      "Epoch 00001: val_acc improved from -inf to 0.82030, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 513s 57ms/step - loss: 0.1355 - acc: 0.8027 - val_loss: 0.1249 - val_acc: 0.8203\n",
      "Epoch 2/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1260 - acc: 0.8192\n",
      "Epoch 00002: val_acc improved from 0.82030 to 0.82441, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 521s 58ms/step - loss: 0.1260 - acc: 0.8192 - val_loss: 0.1221 - val_acc: 0.8244\n",
      "Epoch 3/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1230 - acc: 0.8241\n",
      "Epoch 00003: val_acc improved from 0.82441 to 0.82886, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 464s 51ms/step - loss: 0.1230 - acc: 0.8241 - val_loss: 0.1203 - val_acc: 0.8289\n",
      "Epoch 4/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.8273\n",
      "Epoch 00004: val_acc improved from 0.82886 to 0.83017, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 483s 53ms/step - loss: 0.1212 - acc: 0.8273 - val_loss: 0.1190 - val_acc: 0.8302\n",
      "Epoch 5/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1199 - acc: 0.8294\n",
      "Epoch 00005: val_acc improved from 0.83017 to 0.83212, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 477s 53ms/step - loss: 0.1199 - acc: 0.8294 - val_loss: 0.1179 - val_acc: 0.8321\n"
     ]
    }
   ],
   "source": [
    "model_trained, predictions = run_glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22d87f",
   "metadata": {},
   "source": [
    "### best preprocessing with pretrained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3818181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 50, 50)            1000000   \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          (None, 50, 128)           32128     \n",
      "                                                                 \n",
      " max_pooling1d_28 (MaxPoolin  (None, 25, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 25, 128)           0         \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 25, 64)            49216     \n",
      "                                                                 \n",
      " max_pooling1d_29 (MaxPoolin  (None, 12, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " conv1d_30 (Conv1D)          (None, 12, 32)            14368     \n",
      "                                                                 \n",
      " max_pooling1d_30 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 12, 32)            8224      \n",
      "                                                                 \n",
      " max_pooling1d_31 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 385       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,104,321\n",
      "Trainable params: 104,321\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8025\n",
      "Epoch 00001: val_acc improved from -inf to 0.81980, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 506s 56ms/step - loss: 0.1357 - acc: 0.8025 - val_loss: 0.1248 - val_acc: 0.8198\n",
      "Epoch 2/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.8191\n",
      "Epoch 00002: val_acc improved from 0.81980 to 0.82586, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 517s 57ms/step - loss: 0.1260 - acc: 0.8191 - val_loss: 0.1211 - val_acc: 0.8259\n",
      "Epoch 3/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.8241\n",
      "Epoch 00003: val_acc improved from 0.82586 to 0.82909, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 489s 54ms/step - loss: 0.1230 - acc: 0.8241 - val_loss: 0.1198 - val_acc: 0.8291\n",
      "Epoch 4/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.8269\n",
      "Epoch 00004: val_acc improved from 0.82909 to 0.83208, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 511s 57ms/step - loss: 0.1213 - acc: 0.8269 - val_loss: 0.1175 - val_acc: 0.8321\n",
      "Epoch 5/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1200 - acc: 0.8294\n",
      "Epoch 00005: val_acc improved from 0.83208 to 0.83311, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 517s 57ms/step - loss: 0.1200 - acc: 0.8294 - val_loss: 0.1170 - val_acc: 0.8331\n"
     ]
    }
   ],
   "source": [
    "model_trained_fp, predictions_fp = run_glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288a3d8",
   "metadata": {},
   "source": [
    "### run 2nd best preprocessing with our own glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbc75ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 50, 50)            113057450 \n",
      "                                                                 \n",
      " conv1d_36 (Conv1D)          (None, 50, 128)           32128     \n",
      "                                                                 \n",
      " max_pooling1d_36 (MaxPoolin  (None, 25, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 25, 128)           0         \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (None, 25, 64)            49216     \n",
      "                                                                 \n",
      " max_pooling1d_37 (MaxPoolin  (None, 12, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " conv1d_38 (Conv1D)          (None, 12, 32)            14368     \n",
      "                                                                 \n",
      " max_pooling1d_38 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " conv1d_39 (Conv1D)          (None, 12, 32)            8224      \n",
      "                                                                 \n",
      " max_pooling1d_39 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 385       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,161,771\n",
      "Trainable params: 104,321\n",
      "Non-trainable params: 113,057,450\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.7907- ETA\n",
      "Epoch 00001: val_acc improved from -inf to 0.80938, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 515s 57ms/step - loss: 0.1420 - acc: 0.7907 - val_loss: 0.1315 - val_acc: 0.8094\n",
      "Epoch 2/5\n",
      "9045/9045 [==============================] - ETA: 0s - loss: 0.1311 - acc: 0.8100\n",
      "Epoch 00002: val_acc improved from 0.80938 to 0.81799, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 472s 52ms/step - loss: 0.1311 - acc: 0.8100 - val_loss: 0.1264 - val_acc: 0.8180\n",
      "Epoch 3/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.8156\n",
      "Epoch 00003: val_acc improved from 0.81799 to 0.82144, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 475s 52ms/step - loss: 0.1278 - acc: 0.8156 - val_loss: 0.1243 - val_acc: 0.8214\n",
      "Epoch 4/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.8193\n",
      "Epoch 00004: val_acc improved from 0.82144 to 0.82392, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 502s 55ms/step - loss: 0.1257 - acc: 0.8193 - val_loss: 0.1225 - val_acc: 0.8239\n",
      "Epoch 5/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.8217\n",
      "Epoch 00005: val_acc improved from 0.82392 to 0.82652, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 483s 53ms/step - loss: 0.1242 - acc: 0.8217 - val_loss: 0.1215 - val_acc: 0.8265\n"
     ]
    }
   ],
   "source": [
    "model_trained1, predictions1 = run_glove(vector_size = 50, use_pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd13140",
   "metadata": {},
   "source": [
    "### run best preprocessing with our own glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2315f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 50, 50)            113057450 \n",
      "                                                                 \n",
      " conv1d_32 (Conv1D)          (None, 50, 128)           32128     \n",
      "                                                                 \n",
      " max_pooling1d_32 (MaxPoolin  (None, 25, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 25, 128)           0         \n",
      "                                                                 \n",
      " conv1d_33 (Conv1D)          (None, 25, 64)            49216     \n",
      "                                                                 \n",
      " max_pooling1d_33 (MaxPoolin  (None, 12, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " conv1d_34 (Conv1D)          (None, 12, 32)            14368     \n",
      "                                                                 \n",
      " max_pooling1d_34 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 12, 32)            8224      \n",
      "                                                                 \n",
      " max_pooling1d_35 (MaxPoolin  (None, 12, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 12, 32)            0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 385       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,161,771\n",
      "Trainable params: 104,321\n",
      "Non-trainable params: 113,057,450\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.7916\n",
      "Epoch 00001: val_acc improved from -inf to 0.81232, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 581s 64ms/step - loss: 0.1418 - acc: 0.7916 - val_loss: 0.1293 - val_acc: 0.8123\n",
      "Epoch 2/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.8103\n",
      "Epoch 00002: val_acc improved from 0.81232 to 0.81586, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 573s 63ms/step - loss: 0.1310 - acc: 0.8103 - val_loss: 0.1269 - val_acc: 0.8159\n",
      "Epoch 3/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.8162\n",
      "Epoch 00003: val_acc improved from 0.81586 to 0.82254, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 589s 65ms/step - loss: 0.1275 - acc: 0.8162 - val_loss: 0.1245 - val_acc: 0.8225\n",
      "Epoch 4/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.8197\n",
      "Epoch 00004: val_acc did not improve from 0.82254\n",
      "9045/9045 [==============================] - 582s 64ms/step - loss: 0.1255 - acc: 0.8197 - val_loss: 0.1237 - val_acc: 0.8221\n",
      "Epoch 5/5\n",
      "9044/9045 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.8223\n",
      "Epoch 00005: val_acc improved from 0.82254 to 0.82525, saving model to Embeddings_best_weights.hdf5\n",
      "9045/9045 [==============================] - 580s 64ms/step - loss: 0.1240 - acc: 0.8223 - val_loss: 0.1213 - val_acc: 0.8253\n"
     ]
    }
   ],
   "source": [
    "model_trained2, predictions2 = run_glove(vector_size = 50, use_pretrained=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
