{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the necessary libraries \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pattern                                                          #pip install pattern\n",
    "from pattern.en import lemma, lexeme\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk as nltk\n",
    "import wordninja as wn                                                  #pip install worldninja\n",
    "import matplotlib as plt\n",
    "import seaborn as sns                                                   #pip install seaborn\n",
    "import re\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from gingerit.gingerit import GingerIt                                  #pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the necessary datasets : \n",
    "positive and negative training sets and \n",
    "positive and negative word dictionnaries\n",
    "\"\"\"\n",
    "PATH_TRAIN_NEG = '../Resources/train_neg.txt'\n",
    "PATH_TRAIN_POS = '../Resources/train_pos.txt'\n",
    "\n",
    "PATH_DICT_POS = '../Resources/positive-words.txt'\n",
    "PATH_DICT_NEG = '../Resources/negative-words.txt'\n",
    "\n",
    "with open(PATH_TRAIN_POS) as f:\n",
    "    train_pos = f.read().splitlines()\n",
    "with open(PATH_TRAIN_NEG) as f:\n",
    "    train_neg = f.read().splitlines()\n",
    "with open(PATH_DICT_POS,encoding = \"ISO-8859-1\") as f:\n",
    "    POSITIVE_WORDS_LIST = set((x.strip() for x in f.readlines()))\n",
    "with open(PATH_DICT_NEG,encoding = \"ISO-8859-1\") as f:\n",
    "    NEGATIVE_WORDS_LIST = set((x.strip() for x in f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\julie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\julie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download english stopwords library from the nltk package\n",
    "Initialise stemmer using nltk PorterStemmer function \n",
    "Initialise lemmatizer using nltk WordNetLemmatizer function\n",
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace consecutive punctiation by a specific codeword (consecutive...)\n",
    "Delete all other punctuation, consecutive spaces or special characters\n",
    "\"\"\"\n",
    "def replace_ponctuation(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) : \n",
    "        #replace multiple stops by the word 'consecutivestop'\n",
    "        tweet = re.sub(r\"(\\.)\\1+\", ' consecutiveStop ', tweet)\n",
    "        #replace multiple exclamation by the word 'consecutivequestion'\n",
    "        tweet = re.sub(r\"(\\?)\\1+\", ' consecutiveQuestion ', tweet)\n",
    "        #replace multiple exclamation by the word 'consecutiveexclamation'\n",
    "        tweet = re.sub(r\"(\\!)\\1+\", ' consecutiveExclamation ', tweet)\n",
    "        #delete all ponctuaction\n",
    "        tweet = re.sub(r\"[,.;@?!&$\\\\*\\\"]+\\ *\", ' ', tweet)\n",
    "        #deleting consecutive spaces\n",
    "        tweet = re.sub(r\"\\s+\", ' ',tweet)\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove characters repeted consecutively more than twice to leave only 2 consecutive letters\n",
    "ex : heeeeeeeey -> heey\n",
    "\"\"\"\n",
    "def letter_repetition_treatment(tweet_list) : \n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replaces specific emojies with the word \"positive\" or the word \"negative\"\n",
    "\"\"\"\n",
    "def emoji_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative ', tweet)\n",
    "        # Cry -- :,(, :'(, :\"(\n",
    "        tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative ', tweet)\n",
    "        # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive ', tweet)\n",
    "        # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive ', tweet)\n",
    "        # Love -- <3, :*\n",
    "        tweet = re.sub(r'(<3|:\\*)', ' love ', tweet)\n",
    "        # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "        tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive ', tweet)\n",
    "        tweet_list[i] = tweet\n",
    "\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove the '#' sign for each string and split the words in the hashtag into the most likely combination of words\n",
    "ex : #Machinelearningisthefuture -> Machine learning is the future\n",
    "\"\"\"\n",
    "def hashtag_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = np.array(tweet.split(),dtype='object')\n",
    "        for word in tweet :\n",
    "            if '#' in word :\n",
    "                index = np.where(tweet == word)\n",
    "                word = \" \".join(wn.split(word))\n",
    "                if (isinstance(tweet, str)):\n",
    "                    tweet.replace('#', '')\n",
    "                else :\n",
    "                    np.put(tweet,index[0][0],word)\n",
    "                tweet = \" \".join(tweet)\n",
    "                tweet_list[i]=tweet\n",
    "            \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace current expression in their contracted form with their full form\n",
    "ex : I've gotta go -> I have got to go \n",
    "\"\"\"\n",
    "def apostrophe_contraction(tweet_list) :\n",
    "    \n",
    "    contractions = {\n",
    "        '\\'m' : ' am',\n",
    "        'im' : ' I am',\n",
    "        'ive' : 'I have',\n",
    "        '\\'re' : ' are', \n",
    "        '\\'ve' : ' have',\n",
    "        '\\'s' : ' is', \n",
    "        '\\'ll' : ' will',\n",
    "        '\\'d' : ' would', \n",
    "        '\\'t' : ' not',\n",
    "        'ain\\'t' : 'not',\n",
    "        'aint' : 'not',\n",
    "        'can\\'t' : 'can not',\n",
    "        'cant' : 'can not',\n",
    "        'don\\'t' : 'do not',\n",
    "        'dont' : 'do not',\n",
    "        'isn\\'t' : 'is not',\n",
    "        'isnt' : 'is not',\n",
    "        'won\\'t' : 'will not',\n",
    "        'wont' : 'will not',\n",
    "        'shouldn\\'t' : 'should not',\n",
    "        'shouldnt' : 'should not',\n",
    "        'couldn\\'t' : 'could not',\n",
    "        'wouldn\\'t' : 'would not', \n",
    "        'aren\\'t' : 'are not', \n",
    "        'arent' : 'are not', \n",
    "        'doesn\\'t' : 'does not',\n",
    "        'doesnt' : 'does not',\n",
    "        'wasn\\'t' : 'was not',\n",
    "        'wasnt' : 'was not',\n",
    "        'weren\\'t' : 'were not',\n",
    "        'werent' : 'were not',\n",
    "        'hasn\\'t' : 'has not', \n",
    "        'haven\\'t' : 'have not',\n",
    "        'havent' : 'have not',\n",
    "        'hadn\\'t' : 'had not', \n",
    "        'mustn\\'t' : 'must not', \n",
    "        'didn\\'t' : 'did not', \n",
    "        'mightn\\'t' : 'might not', \n",
    "        'needn\\'t' : 'need not',\n",
    "        'imma' : 'i am going to',\n",
    "        'wanna' : 'want to',\n",
    "        'gonna' : 'going to',\n",
    "        'gotta' : 'got to',\n",
    "        'thats' : 'that is',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(contractions))\n",
    "    tweet_list = [pat.sub(lambda m: contractions.get(m.group()), tweet.lower()) for tweet in tweet_list]\n",
    "    \n",
    "    return [re.sub(r\"\\'\", ' ', tweet) for tweet in tweet_list]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace current slang words with their meaning using our mapping \n",
    "ex : 'cya 2nite gurl' -> 'see you tonight girl' \n",
    "\"\"\"\n",
    "def correct_slang(tweet_list) : \n",
    "    \n",
    "    slang = {\n",
    "        '2nite' : 'tonight',\n",
    "        '2night' : 'tonight',\n",
    "        '2' : 'to',\n",
    "        '4' : 'for',\n",
    "        'ab' : 'about',\n",
    "        'ace' : 'success',\n",
    "        'ad' : 'awesome person',\n",
    "        'af' : 'very', #mmmh could do better : word af  -> very word maybe ? \n",
    "        'aka' : 'meaning',\n",
    "        'asap' : 'soon',\n",
    "        'aww' : 'cute',\n",
    "        'bc' : 'because',\n",
    "        'bf' : 'boyfriend',\n",
    "        'bff' : 'best friend',\n",
    "        'brb' : 'I come',\n",
    "        'btw' : 'by the way',\n",
    "        'cus' : 'because',\n",
    "        'cuz' : 'because',\n",
    "        'cya' : 'see you',\n",
    "        'dammit' : 'damn it',\n",
    "        'dam' : 'damn',\n",
    "        'der' : 'there',\n",
    "        'dm' : 'message me',\n",
    "        'dunno' : 'do not know',\n",
    "        'dnt' : 'do not',\n",
    "        'dw' : 'okay',\n",
    "        'ew' : 'gross',\n",
    "        'ftw' : 'win',\n",
    "        'fyi' : 'for information',\n",
    "        'gf' : 'girlfriend',\n",
    "        'gotta' : 'has',\n",
    "        'gurl' : 'girl',\n",
    "        'haha' : 'laught',\n",
    "        'hahah' : 'laught',\n",
    "        'hahaha' : 'laught',\n",
    "        'hahahah' : 'laught',\n",
    "        'hahahaha' : 'laught',\n",
    "        'hmu' : 'message me',\n",
    "        'idk' : 'do not know',\n",
    "        'idc' : 'do not care',\n",
    "        'ily' : 'love',\n",
    "        'imo' : 'think',\n",
    "        'irl' : 'real life',\n",
    "        'jk' : 'laught',\n",
    "        'lmao' : 'laught',\n",
    "        'lmk' : 'let me know',\n",
    "        'lil' : 'little',\n",
    "        'lol' : 'laught',\n",
    "        'luv' : 'love',\n",
    "        'ppl' : 'people',\n",
    "        'n' : 'and',\n",
    "        'nbd' : 'okay', #no big deal\n",
    "        'np' : 'okay', #no problem\n",
    "        'nvm' : 'okay', #never mind\n",
    "        'omg' : 'amazing', #oh my god\n",
    "        'omw' : \"come\",\n",
    "        'r' : 'are',\n",
    "        'rofl' : 'laught',\n",
    "        'roflmao' : 'laught',\n",
    "        'rn' : 'now',\n",
    "        'rt' : 'retweet',\n",
    "        'sch' : 'school',\n",
    "        'tbh' : 'honestly',\n",
    "        'til' : 'until',\n",
    "        'thx' : 'thanks',\n",
    "        'ttyl' : 'talk later',\n",
    "        'u' : 'you',\n",
    "        'ur' : 'your',\n",
    "        'w' : 'with',\n",
    "        'wan' : 'want',\n",
    "        'waz' : 'what is',\n",
    "        'wtf' : 'seriously',\n",
    "        'x' : 'kiss',\n",
    "        'xx' : 'kiss',\n",
    "        'xo' : 'kiss',\n",
    "        'xoxo' : 'kiss',\n",
    "        'xd' : 'laught',\n",
    "        'y' : 'why',\n",
    "        'ya' : 'you',\n",
    "        'yay' : 'happy',\n",
    "        'yolo' : 'enjoy',\n",
    "        'yuck' : 'gross',\n",
    "    }\n",
    "    pat = re.compile(r\"\\b(%s)\\b\" % \"|\".join(slang))\n",
    "\n",
    "    return [pat.sub(lambda m: slang.get(m.group()), tweet.lower()) for tweet in tweet_list]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace current slang words with their meaning using gingerIt package \n",
    "\"\"\"\n",
    "def correct_slang2(tweet_list) : \n",
    "    \n",
    "    parser = GingerIt()\n",
    "    for i, tweet in enumerate(tweet_list) : \n",
    "        t = parser.parse(tweet)\n",
    "        tweet_list[i] = t.get('result')\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove words that have length one \n",
    "\"\"\"\n",
    "def short_word_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = \" \".join([word for word in tweet.split() if len(word) > 1])\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove numbers and possible remaining punctuation / special characters\n",
    "\"\"\"\n",
    "def numbers_treatment(tweet_list):\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list): \n",
    "        new_tweet = []\n",
    "        for word in tweet.split():\n",
    "            try:\n",
    "                word = re.sub('[,\\.:%_\\-\\+\\*\\/\\%\\_]', '', word)\n",
    "                float(word)\n",
    "                new_tweet.append(\"\")\n",
    "            except:\n",
    "                new_tweet.append(word)\n",
    "            tweet_list[i] = \" \".join(new_tweet)\n",
    "            \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace words that are not uniquely composed of alphabetic characters (ie contain numbers or special characters) \n",
    "\"\"\"\n",
    "def non_alphabetic_treatment(tweet_list) : \n",
    "    \n",
    "    for i, tweet in enumerate(tweet_list) :\n",
    "        tweet = \" \".join([word for word in tweet.split() if word.isalpha()])\n",
    "        tweet_list[i] = tweet\n",
    "\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Use spell corrector dictionnary from ekphrasis package to correct spelling mistakes\n",
    "\"\"\"\n",
    "def correct_spelling(tweet_list):\n",
    "    \n",
    "    sp = SpellCorrector(corpus=\"english\")\n",
    "    \n",
    "    return [sp.correct_text(tweet) for tweet in tweet_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove english stopwords excluding the one that may help understand the polarity of the tweet\n",
    "ex : this processing step is an important step -> proccessing step important step\n",
    "\"\"\"\n",
    "def stopwords_treatment(tweet_list) :\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.difference_update(['no','not','but','why','won','won\\'t','very','don\\'t','against'])\n",
    "    \n",
    "    return [\" \".join(w for w in tweet.split() if not w in stop_words) for tweet in tweet_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Remove suffix of words \n",
    "ex : studies study studying studied -> studi studi studi studi\n",
    "\"\"\"\n",
    "def stemming_treatment(tweet_list):\n",
    "    \n",
    "    for i,tweet in enumerate(tweet_list) :\n",
    "        tweet =  \" \".join(stemmer.stem(t) for t in tweet.split())\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with wordNet but not that great\n",
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace words with the root of the word \n",
    "ex : studies study studying studied -> study study studying studied\n",
    "\"\"\"\n",
    "def lemmatizing_treatment(tweet_list):\n",
    "    \n",
    "    for i,tweet in enumerate(tweet_list) :\n",
    "        tweet =  \" \".join(lemmatizer.lemmatize(t) for t in tweet.split())\n",
    "        tweet_list[i] = tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now with Pattern lemmatizer : seems to work great ! \n",
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Replace words with the root of the word \n",
    "ex : studies study studying studied -> study study study study\n",
    "\"\"\"\n",
    "def lemmatizing_treatment2(tweet_list) : \n",
    "    \n",
    "    for i,tweet in enumerate(tweet_list) :\n",
    "        new_tweet =  \" \".join([lemma(word) for word in tweet.split()])\n",
    "        tweet_list[i] = new_tweet\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : tweet_list (list of strings)\n",
    "Use dictionnaries to identify positive and negative words in string,\n",
    "append the word \"positive\" or \"negative\" to the word in question in the string\n",
    "ex : great zombie are here -> positive great negative zombie are here\n",
    "\"\"\"\n",
    "def negative_positive_word_treatment(tweet_list) :\n",
    "    \n",
    "    def check_word(word): \n",
    "        \n",
    "        if word in POSITIVE_WORDS_LIST :\n",
    "            return \"positive \" + word\n",
    "        elif word in NEGATIVE_WORDS_LIST :\n",
    "            return \"negative \" + word\n",
    "        \n",
    "        return word\n",
    "\n",
    "    return [\" \".join(check_word(w) for w in tweet.split()) for tweet in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = replace_ponctuation(train_pos)\n",
    "train_neg = replace_ponctuation(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = letter_repetition_treatment(train_pos)\n",
    "train_neg = letter_repetition_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = emoji_treatment(train_pos)\n",
    "train_neg = emoji_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = hashtag_treatment(train_pos)\n",
    "train_neg = hashtag_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = apostrophe_contraction(train_pos)\n",
    "train_neg = apostrophe_contraction(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = correct_slang(train_pos)\n",
    "train_neg = correct_slang(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = correct_slang2(train_pos)\n",
    "#train_neg = correct_slang2(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = short_word_treatment(train_pos)\n",
    "train_neg = short_word_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = numbers_treatment(train_pos)\n",
    "train_neg = numbers_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = non_alphabetic_treatment(train_pos)\n",
    "train_neg = non_alphabetic_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = correct_spelling(train_pos)\n",
    "#train_neg = correct_spelling(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = stopwords_treatment(train_pos)\n",
    "train_neg = stopwords_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = stemming_treatment(train_pos)\n",
    "#train_neg = stemming_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = lemmatizing_treatment(train_pos)\n",
    "#train_neg = lemmatizing_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = lemmatizing_treatment2(train_pos)\n",
    "train_neg = lemmatizing_treatment2(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = negative_positive_word_treatment(train_pos)\n",
    "train_neg = negative_positive_word_treatment(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : train_pos, train_neg (training datasets)\n",
    "add second column to training sets to identify tweets from dataset :) (1) and from dataset :( (-1)\n",
    "\"\"\"\n",
    "def label_data(train_pos,train_neg):\n",
    "    \n",
    "    train_pos = np.array(train_pos).reshape(-1,1)\n",
    "    ones = np.ones(shape=(train_pos.shape[0],1))\n",
    "    train_pos = np.concatenate((train_pos,ones),axis = 1)\n",
    "\n",
    "    train_neg = np.array(train_neg).reshape(-1,1)\n",
    "    neg_ones = np.zeros(shape=(train_neg.shape[0],1))-1\n",
    "    train_neg = np.concatenate((train_neg,neg_ones),axis = 1)\n",
    "    \n",
    "    return (train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos,train_neg = label_data(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of \\<user> and \\<url> on the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : train_pos, train_neg (training datasets)\n",
    "compare the presence of the <user> tag in the positive training set versus the negative set \n",
    "to determine whether we can remove it or not\n",
    "\"\"\"\n",
    "def user_tag_impact(train_pos,train_neg):\n",
    "    \n",
    "    user = \"<user>\"\n",
    "    user_count_pos = 0\n",
    "    user_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if user in train_pos[i] :\n",
    "            user_count += 1\n",
    "            user_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if user in train_neg[i] :\n",
    "            user_count += 1\n",
    "            \n",
    "    user_count_neg = user_count - user_count_pos\n",
    "    counts = np.array([user_count,user_count_pos,user_count_neg])\n",
    "\n",
    "    user_dict = {\"Positive Sentiment Tweet\":user_count_pos,\"Negative Sentiment Tweet\":user_count_neg}\n",
    "    keys = list(user_dict.keys())\n",
    "    vals = [user_dict[k] for k in keys]\n",
    "    ax1 = sns.barplot(x=keys, y=vals)   \n",
    "    ax1.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax1.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax1.set_title(\"User Tag Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tag_counts = url_impact(train_pos,train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argument : train_pos, train_neg (training datasets)\n",
    "compare the presence of the <url> tag in the positive training set versus the negative set \n",
    "to determine whether we can remove it or not\n",
    "\"\"\"\n",
    "def url_impact(train_pos,train_neg):\n",
    "    \n",
    "    url = \"<url>\"\n",
    "    url_count_pos = 0\n",
    "    url_count = 0\n",
    "    \n",
    "    for i in range(len(train_pos)):\n",
    "        if url in train_pos[i] :\n",
    "            url_count += 1\n",
    "            url_count_pos += 1\n",
    "            \n",
    "    for i in range(len(train_neg)):\n",
    "        if url in train_neg[i] :\n",
    "            url_count += 1\n",
    "            \n",
    "    url_count_neg = url_count - url_count_pos\n",
    "    counts = np.array([url_count,url_count_pos,url_count_neg])\n",
    "    \n",
    "    url_dict = {\"Positive Sentiment Tweet\":url_count_pos,\"Negative Sentiment Tweet\":url_count_neg}\n",
    "    keys = list(url_dict.keys())\n",
    "    vals = [url_dict[k] for k in keys]\n",
    "    ax = sns.barplot(x=keys, y=vals)   \n",
    "    ax.set_xlabel(\"Sentiment type\", fontsize = 10)\n",
    "    ax.set_ylabel(\"Number of Tweets\", fontsize = 10)\n",
    "    ax.set_title(\"Url Presence impact on Tweet Sentiment\",fontsize = 20,pad=25)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tag_counts = user_tag_impact(train_pos,train_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
